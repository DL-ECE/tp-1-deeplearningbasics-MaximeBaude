{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    },
    "colab": {
      "name": "rapport.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DL-ECE/tp-1-deeplearningbasics-MaximeBaude/blob/master/rapport.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL6tB583uGtl"
      },
      "source": [
        "# TP-1 DLBasics\n",
        "\n",
        "## Digit classification using the MNIST dataset\n",
        "\n",
        "In this notebook you will train your first neural network. Feel free to look back at the Lecture-1 slides to complete the cells below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVSaOUITuGts"
      },
      "source": [
        "#### Install dependencies freeze by poetry \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdz_4FIwuGuK"
      },
      "source": [
        "#### Import the different module we will need in this notebook \n",
        "\n",
        "All the dependencies are installed. Below we import them and will be using them in all our notebooks.\n",
        "\n",
        "Please feel free to look arround and look at their API. \n",
        "\n",
        "The student should be limited to these imports to complete this work.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNpBRHQKuGuP"
      },
      "source": [
        "# We import some python standard librairy utility function \n",
        "# see the [python doc](https://docs.python.org/3.6/library/functools.html?highlight=func#module-functools) for more info \n",
        "from functools import reduce \n",
        "import random \n",
        "\n",
        "# To create some plot and figures: matplolib [matplotlib doc](https://matplotlib.org/)\n",
        "# To do compute on matrix and vectors: [numpy doc](https://numpy.org/)\n",
        "# To do some classical Machine Learning: [sklearn doc](https://scikit-learn.org/stable/index.html)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 304,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88ES94TeuGue"
      },
      "source": [
        "# In order to have some reproducable results and easier debugging \n",
        "# we fix the seed of random.\n",
        "random.seed(1342)\n",
        "np.random.seed(1342)"
      ],
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNDpQvrpuGuu"
      },
      "source": [
        "## Data preparation (3 pts)\n",
        "\n",
        "As seen in the lecture one of the earlier use case for deep learning was digit recognition. \n",
        "\n",
        "The dataset we will use today is the MNISTdataset http://yann.lecun.com/exdb/mnist/. \n",
        "\n",
        "One image will be represented a vector (a 28x28 image will be represented as vector with 784 entries).\n",
        "\n",
        "Thus, we will end up with a n_examples x 784 matrix to represent the images in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8GMPq2YuGuw"
      },
      "source": [
        "mnist_data, mnist_target = fetch_openml('mnist_784', version=1, return_X_y=True)"
      ],
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vLohguwuGvC"
      },
      "source": [
        "# Let's warmup and answer this first question\n",
        "# Replace the None with you answer.\n",
        "\n",
        "# How many image are in this dataset ? \n",
        "def data_length(dataset: np.array, target: np.array):\n",
        "    \"\"\"Function to compute the length of the dataset and the length of the target labels.\"\"\"\n",
        "    dataset_length = dataset.shape[0]\n",
        "    target_length = target.shape[0]\n",
        "    return dataset_length, target_length\n",
        "\n",
        "    data_length(mnist_data,mnist_target)"
      ],
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ej4a7mMuGvQ",
        "outputId": "f06f4c0d-66dc-4f6c-ffad-1218e78ea6b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# Let's look at on image from this dataset \n",
        "def plot_one_image(dataset: np.array, target: np.array, image_index: int):\n",
        "    \"\"\"Function to plot the image at the given index.\"\"\"\n",
        "    image = dataset[image_index].reshape(28,28)\n",
        "    target = target[image_index]\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(f\"This is a {target}\")\n",
        "\n",
        "\n",
        "plot_one_image(mnist_data, mnist_target ,0)"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGSW67l4uGvc"
      },
      "source": [
        "\n",
        "# In a similar fashion to classical machine learning, we will create a test split to known if the neural network is learning well.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(mnist_data, mnist_target, test_size=0.33, random_state=1342)\n",
        "# You the 2 function below to check if they are working properly on this divided dataset.\n",
        "\n",
        "X_train_length, y_train_length = data_length(X_train, y_train)\n",
        "X_test_length, y_test_length = data_length(X_test, y_test)\n",
        "\n",
        "assert X_train_length == y_train_length and X_train_length == 46900\n",
        "assert X_test_length == y_test_length and X_test_length == 23100\n"
      ],
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smKaWKfouGvr",
        "outputId": "fb0f2e1f-6ad0-4601-d9b4-bffd8607510a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plot_one_image(X_train, y_train , 120)"
      ],
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uUh3bUOuGv2",
        "outputId": "e280f95f-7cec-493b-d30d-e0641be1e5d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plot_one_image(X_test, y_test , 250)"
      ],
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQNUlEQVR4nO3de4yVdX7H8fdHBHS5VSuOeOmyxdvSjauGgqZUdL2768KmiZFGl1qboVZDTTRZd5tUbK2xza4NicmaUcjiekEUUbLBdV0j4oa4CugiKqiYoTDhIqFdsFVB+PaP89COOud3Zs7d+X1eyWTOeb7P5csJn3mec57zPD9FBGY2+B3W6gbMrDkcdrNMOOxmmXDYzTLhsJtlwmE3y4TDPghImivpoUT9TUnnD3Cdfy5pY83NWds4vNUNWGWSPuz19CvAJ8CB4vnsSstHxJ8MdJsR8RJw2kCXGyhJXwF+DFwFDAV+FxHnNXq7OXLYvwQiYuShx5K6gb+JiF/3mja3BW3VSxel/4dfB3YDZ7a2ncHLh/GDxzBJD0raWxy2TzpUkNQt6aLi8WRJqyXtkbRD0j19rUzS+ZK29nr+A0k9xfo3SrqwzHLflvRasf4tqT9Ekk4Hvgt0RsQHEXEgItZU+e+3Chz2weO7wCLgD4BlwL1l5psHzIuI0cAEYHGlFUs6DbgJ+NOIGAVcCnSXmf2/ge8XfXwbuEHSjDLzTgY2A3dI2iXpDUl/Uakfq47DPnj8JiKWR8QB4OfAN8vMtx84WdIxEfFhRLzcj3UfAIYDEyUNjYjuiNjU14wRsSIi3oiIgxGxDngUmFZmvScC3wB+DxxP6Q/KQklf70dPNkAO++Cxvdfj/wGOkNTXZzLXA6cCGyS9Kuk7lVYcEe8BNwNzgZ2SFkk6vq95JU2R9IKkDyT9Hvhb4Jgyq/6I0h+fOyNiX0S8CLwAXFKpJxs4hz0zEfFuRMwEjgX+FXhC0oh+LPdIREwFvgpEsWxfHqH0NuKkiBgD3AeozLzr+tpUpV6sOg57ZiRdI2lsRBwE/quYfLDCMqdJ+pak4cDHlPbI5ZYZBeyOiI8lTQb+MrHqlcB/AD+UdLikPwMuAJ4dwD/J+slhz89lwJvFuft5wNUR8VGFZYYDdwO7KL1dOBb4YZl5/w74J0l7gX8k8QFgROwHpgNXUHrffj/w/YjY0P9/jvWXfPMKszx4z26WCYfdLBMOu1kmHHazTDT1QhhJ/jTQrMEios/vNdS0Z5d0WXFRxHuSbqtlXWbWWFWfepM0BHgHuBjYCrwKzIyItxLLeM9u1mCN2LNPBt6LiPcjYh+lK66m17A+M2ugWsJ+ArCl1/OtxbTPkNRZXD+9uoZtmVmNGv4BXUR0UbobiQ/jzVqolj17D3BSr+cnFtPMrA3VEvZXgVMkfU3SMOBqSpc2mlkbqvowPiI+lXQTpcsRhwALIuLNunVmZnXV1Kve/J7drPEa8qUaM/vycNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulomm3krarLeTTz45WX/ooYeS9SlTpiTrK1asKFu74IILkssORt6zm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZ8Hl2a6hLL720bO2xxx5LLjtq1Khk/eDBg8n6smUexqA379nNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0z4PLslHXHEEcn6xRdfnKw//vjjZWtDhw5NLrt8+fJkfcGCBcn6U089laznpqawS+oG9gIHgE8jYlI9mjKz+qvHnv2CiNhVh/WYWQP5PbtZJmoNewC/krRGUmdfM0jqlLRa0uoat2VmNaj1MH5qRPRIOhZ4TtKGiFjZe4aI6AK6ACRFjdszsyrVtGePiJ7i905gKTC5Hk2ZWf1VHXZJIySNOvQYuARYX6/GzKy+ajmM7wCWSjq0nkci4pd16cqa5rDD0n/v77vvvmT92muvrXrb8+fPT9bvuuuuZL27u7vqbeeo6rBHxPvAN+vYi5k1kE+9mWXCYTfLhMNulgmH3SwTDrtZJnyJa+bmzJmTrNdyag2gq6ur6m3v37+/pm3bZ3nPbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlQhHNu3mM71TTfJdffnmyvmjRomR95MiRyXql2znPnj27bK3SkMtWnYhQX9O9ZzfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuHz7IPA6NGjy9ZefPHF5LJnnHFGsr5q1apk/aKLLkrWP/nkk2Td6s/n2c0y57CbZcJhN8uEw26WCYfdLBMOu1kmHHazTPi+8YPAkiVLytYqnUdfvnx5sn7llVdW1ZO1n4p7dkkLJO2UtL7XtKMlPSfp3eL3UY1t08xq1Z/D+J8Bl31u2m3A8xFxCvB88dzM2ljFsEfESmD35yZPBxYWjxcCM+rcl5nVWbXv2TsiYlvxeDvQUW5GSZ1AZ5XbMbM6qfkDuoiI1AUuEdEFdIEvhDFrpWpPve2QNA6g+L2zfi2ZWSNUG/ZlwKzi8Szg6fq0Y2aNUvEwXtKjwPnAMZK2ArcDdwOLJV0PbAauamSTuRs/fnyyftZZZ1W97k2bNlW9rH25VAx7RMwsU7qwzr2YWQP567JmmXDYzTLhsJtlwmE3y4TDbpYJX+L6JTBt2rRk/aijqr/ocNeuXVUva18u3rObZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZpnwkM1tYOzYscn6xo0bk/UxY8aUrW3fvj257KRJk5L1bdu2JevWfjxks1nmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCV/P3gaGDRuWrKfOo1dy6623JuuVzqMff/zxyfq5556brJ933nnJesorr7ySrK9cuTJZ37JlS9XbHoy8ZzfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuHr2dvADTfckKzfe++9Va97w4YNyfoHH3yQrJ966qnJekdHx4B7qpe1a9cm66lz/B999FG922kbVV/PLmmBpJ2S1veaNldSj6TXi58r6tmsmdVffw7jfwZc1sf0f4+IM4uf5fVty8zqrWLYI2IlsLsJvZhZA9XyAd1NktYVh/llBxuT1ClptaTVNWzLzGpUbdh/CkwAzgS2AT8pN2NEdEXEpIhI39nQzBqqqrBHxI6IOBARB4H7gcn1bcvM6q2qsEsa1+vp94D15eY1s/ZQ8Xp2SY8C5wPHSNoK3A6cL+lMIIBuYHYDexz0xo0bV3mmKp1++uk11fft25esb968ecA9HXLccccl68OHD0/Wzz777GT9lltuKVu78847k8sORhXDHhEz+5g8vwG9mFkD+euyZplw2M0y4bCbZcJhN8uEw26WCd9Kug2MHz++Yetes2ZNsv7ss88m688880yyvmrVqgH3dMiUKVOS9QceeCBZnzhxYrLe09Mz4J4GM+/ZzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNM+Dx7E0yYMCFZnzFjRk3r37NnT9natGnTksu28pbKlS6vrXTp7zvvvJOsP/HEEwPuaTDznt0sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TPszfBpk2bkvWPP/44WR8xYkSyPmTIkLK1MWPGJJdt9Hn222+/vWztxhtvTC575JFHJutz585N1vfu3Zus58Z7drNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sE4qI9AzSScCDQAelIZq7ImKepKOBx4DxlIZtvioi/rPCutIby9R1112XrFe6f3rKww8/nKy/9tpryfrSpUtrWv8555xTtnbgwIHkstdcc02yvnjx4mQ9VxGhvqb3Z8/+KXBLREwEzgFulDQRuA14PiJOAZ4vnptZm6oY9ojYFhFri8d7gbeBE4DpwMJitoVAbbdbMbOGGtB7dknjgbOA3wIdEbGtKG2ndJhvZm2q39+NlzQSWALcHBF7pP9/WxARUe79uKROoLPWRs2sNv3as0saSinoD0fEk8XkHZLGFfVxwM6+lo2IroiYFBGT6tGwmVWnYthV2oXPB96OiHt6lZYBs4rHs4Cn69+emdVLf069TQVeAt4ADhaTf0Tpffti4I+AzZROve2usC6feuvD2LFjk/UVK1Yk65VuydxKL7/8ctnavHnzksv61Fp1yp16q/iePSJ+A/S5MHBhLU2ZWfP4G3RmmXDYzTLhsJtlwmE3y4TDbpYJh90sExXPs9d1Yz7PXpXRo0cn63fccUfZ2pw5c2radqVz/Bs2bEjWU9uvdImrVaeWS1zNbBBw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmfJ7dbJDxeXazzDnsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMVwy7pJEkvSHpL0puS/r6YPldSj6TXi58rGt+umVWr4s0rJI0DxkXEWkmjgDXADOAq4MOI+HG/N+abV5g1XLmbVxzejwW3AduKx3slvQ2cUN/2zKzRBvSeXdJ44Czgt8WkmyStk7RA0lFllumUtFrS6po6NbOa9PsedJJGAi8C/xIRT0rqAHYBAfwzpUP9v66wDh/GmzVYucP4foVd0lDgF8CzEXFPH/XxwC8i4hsV1uOwmzVY1TeclCRgPvB276AXH9wd8j1gfa1Nmlnj9OfT+KnAS8AbwMFi8o+AmcCZlA7ju4HZxYd5qXV5z27WYDUdxteLw27WeL5vvFnmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8tExRtO1tkuYHOv58cU09pRu/bWrn2Be6tWPXv7arlCU69n/8LGpdURMallDSS0a2/t2he4t2o1qzcfxptlwmE3y0Srw97V4u2ntGtv7doXuLdqNaW3lr5nN7PmafWe3cyaxGE3y0RLwi7pMkkbJb0n6bZW9FCOpG5JbxTDULd0fLpiDL2dktb3mna0pOckvVv87nOMvRb11hbDeCeGGW/pa9fq4c+b/p5d0hDgHeBiYCvwKjAzIt5qaiNlSOoGJkVEy7+AIek84EPgwUNDa0n6N2B3RNxd/KE8KiJ+0Ca9zWWAw3g3qLdyw4z/FS187eo5/Hk1WrFnnwy8FxHvR8Q+YBEwvQV9tL2IWAns/tzk6cDC4vFCSv9Zmq5Mb20hIrZFxNri8V7g0DDjLX3tEn01RSvCfgKwpdfzrbTXeO8B/ErSGkmdrW6mDx29htnaDnS0spk+VBzGu5k+N8x427x21Qx/Xit/QPdFUyPibOBy4MbicLUtRek9WDudO/0pMIHSGIDbgJ+0splimPElwM0Rsad3rZWvXR99NeV1a0XYe4CTej0/sZjWFiKip/i9E1hK6W1HO9lxaATd4vfOFvfzfyJiR0QciIiDwP208LUrhhlfAjwcEU8Wk1v+2vXVV7Net1aE/VXgFElfkzQMuBpY1oI+vkDSiOKDEySNAC6h/YaiXgbMKh7PAp5uYS+f0S7DeJcbZpwWv3YtH/48Ipr+A1xB6RP5TcA/tKKHMn39MfC74ufNVvcGPErpsG4/pc82rgf+EHgeeBf4NXB0G/X2c0pDe6+jFKxxLeptKqVD9HXA68XPFa1+7RJ9NeV189dlzTLhD+jMMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0z8L7p1I6BNj9izAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo9li-rduGv_"
      },
      "source": [
        "# It's important to normalize the data before feeding it into the neural network\n",
        "def normalize_data(dataset: np.array) -> np.array:\n",
        "    normalized_dataset = (dataset)/(np.max(dataset))\n",
        "    return normalized_dataset\n",
        "\n",
        "    Normalized_data_value=normalize_data(mnist_data)"
      ],
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP_pKu45uGwM"
      },
      "source": [
        "It's also important to find a good representation of the target.\n",
        "\n",
        "In this notebook it will be one-hot vector. \n",
        "\n",
        "Complete the below function to turn the target vector into a one-hot matrix.\n",
        "\n",
        "For example, a `[0,1,9]` vector will become the following matrix:\n",
        "\n",
        "`[[1,0,0,0,0,0,0,0,0,0],\n",
        "  [0,1,0,0,0,0,0,0,0,0],\n",
        "  [0,0,0,0,0,0,0,0,0,1]]`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQrL9OYCuGwO",
        "outputId": "6fe2cee5-8328-4621-8c03-7bab17ba25df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def target_to_one_hot(target: np.array) -> np.array:\n",
        "    target=target.astype(int)\n",
        "    one_hot_matrix = np.zeros([target.shape[0], 10])\n",
        "\n",
        "    one_hot_matrix[np.arange(len(target)), target] = 1 \n",
        "    ###\n",
        "    return one_hot_matrix\n",
        "\n",
        "target_to_one_hot(mnist_target)"
      ],
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 313
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwzVeZv8uGwa"
      },
      "source": [
        "## Useful functions (3 pts)\n",
        "\n",
        "Implement the sigmoid function, its derivative and the softmax function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n3muOriuGwc"
      },
      "source": [
        "def sigmoid(M: np.array) -> np.array:\n",
        "    \"\"\"Apply a sigmoid to the input array\"\"\"\n",
        "    # TODO\n",
        "    return 1/(1 + np.exp(-M))"
      ],
      "execution_count": 314,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GGfWD0duGwr"
      },
      "source": [
        "def d_sigmoid(M: np.array)-> np.array:\n",
        "    \"\"\"Compute the derivative of the sigmoid\"\"\" \n",
        "    # TODO\n",
        "    return sigmoid(M) * (1 - sigmoid(M))"
      ],
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul2LN3-QuGwz"
      },
      "source": [
        "def softmax(X: np.array)-> np.array:\n",
        "    \"\"\"Apply a softmax to the input array\"\"\"\n",
        "    # TODO\n",
        "    return (np.exp(X)) / (np.sum(np.exp(X), axis=1).reshape(-1, 1));"
      ],
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQSQhg7SuGw-"
      },
      "source": [
        "## Feed forward NN\n",
        "\n",
        "Now that the data is prepared it's time to create a neural network to learn on this dataset.\n",
        "\n",
        "You can look back at the lecture slides and need to replace the None in the below function in order to have the building blocks of this first neural network. \n",
        "\n",
        "To do so we are now going to create the FFNN class. It will take list of integers to represent the network.\n",
        "\n",
        "One element in the list corresponds to the number of neurones in the layer.\n",
        "`config = [784, 3, 4, 10]` will be an acceptable config: \n",
        "- inputs are 1x784 vectors \n",
        "- the model output should be a vector of size 10 to classify between 10 classes.\n",
        "- in the middle the hidden layer are fully customizable\n",
        "\n",
        "You have to do some implementations and replace the None assignment (variable = None). Do not do it for the Layer object.\n",
        "\n",
        "Warning: None return type for some methods are not supposed to be affected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBXvlYaGuGxA"
      },
      "source": [
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.Z = None\n",
        "        self.W = None\n",
        "        self.D = None\n",
        "        self.F = None\n",
        "        self.activation = None"
      ],
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kee3Pn3VuGxL"
      },
      "source": [
        "class FFNN:\n",
        "    def __init__(self, config, minibatch_size=100, learning_rate=0.1):\n",
        "        self.layers = []\n",
        "        self.nlayers = len(config)\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        input_data = Layer()\n",
        "        # TODO: initialize the Z matrix with the a matrix containing only zeros\n",
        "        # its shape should be (minibatch_size, config[0])\n",
        "        input_data.Z = np.zeros((minibatch_size, config[0]))\n",
        "        self.layers.append(input_data)\n",
        "                                        \n",
        "        for i in range(1, len(config)):\n",
        "            nnodes = config[i]\n",
        "            layer  = Layer()\n",
        "            nlines_prev, ncols_prev = self.layers[i - 1].Z.shape\n",
        "            # TODO: initilize the weight matrix W in the layer with a random normal distribution\n",
        "            # its shape should be (ncols_prev, nnodes)\n",
        "            layer.W = np.random.randn(ncols_prev, nnodes)\n",
        "            # TODO: initilize the matrix Z in the layer with a matrix containing only zeros\n",
        "            # its shape should be (nlines_prev, nnodes)\n",
        "            layer.Z = np.zeros((nlines_prev, nnodes))\n",
        "            # TODO: use the sigmoid activation function\n",
        "            layer.activation = sigmoid\n",
        "            self.layers.append(layer)\n",
        "        # TODO: Your last layer activation should be a softmax\n",
        "        self.layers[-1].activation = softmax\n",
        "        \n",
        "    def one_step_forward(self, signal: np.array, cur_layer: Layer)-> np.array:\n",
        "        # Compute the F and Z matrix for the current layer and return Z\n",
        "        # TODO: Compute the dot product betzeen the signal and the current layer W matrix\n",
        "        S = np.dot(signal, cur_layer.W)\n",
        "        # TODO: Compute the F matrix of the current layer\n",
        "        cur_layer.F = np.transpose(d_sigmoid(S))\n",
        "        # Compute the activation od the current layer\n",
        "        cur_layer.Z = cur_layer.activation(S)\n",
        "        return cur_layer.Z\n",
        "       \n",
        "    def forward_pass(self, input_data: np.array)-> np.array:\n",
        "        # TODO: perform the whole forward pass using the on_step_forward function\n",
        "        self.layers[0].Z = input_data;\n",
        "        for i in range(1, self.nlayers):\n",
        "          self.layers[i].Z = self.one_step_forward(self.layers[i-1].Z, self.layers[i])\n",
        "        pass\n",
        "        return self.layers[-1].Z\n",
        "    \n",
        "    def one_step_backward(self, prev_layer: Layer, cur_layer: Layer)-> Layer:\n",
        "        # TODO: Compute the D matrix of the current layer using the previous layer and return the current layer\n",
        "        Di = cur_layer.F * np.dot(prev_layer.W, prev_layer.D)\n",
        "        cur_layer.D = Di\n",
        "        return cur_layer.D\n",
        "        \n",
        "    def backward_pass(self, D_out: np.array)-> None:\n",
        "        self.layers[-1].D = np.transpose(D_out)\n",
        "        # TODO: Compute the D matrix for all the layers (excluding the first one which corresponds to the input itself)\n",
        "        # (you should only use self.layers[1:])\n",
        "        for i in range(len(self.layers[1:])-1, 0, -1):\n",
        "          self.layers[i].D = self.one_step_backward(self.layers[i+1], self.layers[i])\n",
        "        pass\n",
        "    \n",
        "    def update_weights(self, cur_layer: Layer, next_layer: Layer)-> Layer:\n",
        "        # TODO: Update the W matrix of the next_layer using the current_layer and the learning rate\n",
        "        # and return the next_layer\n",
        "        next_layer.W = next_layer.W-self.learning_rate*(np.transpose(np.dot(next_layer.D, cur_layer.Z)))\n",
        "        return next_layer.W\n",
        "    \n",
        "    def update_all_weights(self)-> None:\n",
        "        # TODO: Update all W matrix using the update_weights function\n",
        "        for i in range(1, self.nlayers-1):\n",
        "          self.layers[i].W = self.update_weights(self.layers[i-1], self.layers[i])\n",
        "        pass\n",
        "        \n",
        "    def get_error(self, y_pred: np.array, y_batch: np.array)-> float:\n",
        "        # TODO: return the accuracy on the predictions\n",
        "        # the accuracy should be in the [0.0, 1.0] range\n",
        "        er = 0\n",
        "        for i in range(0, len(y_pred)):\n",
        "            i_pred = np.argmax(y_pred[i])\n",
        "            i_batch = np.argmax(y_batch[i])\n",
        "            if(i_pred == i_batch):\n",
        "                er += 1\n",
        "        return er/len(y_pred)\n",
        "    \n",
        "    def get_test_error(self, X: np.array, y: np.array)-> float:\n",
        "        # TODO: Compute the accuracy using the get_error function\n",
        "        nbatch = X.shape[0]\n",
        "        error_sum_train = 0.0\n",
        "        for i in range(0, nbatch):\n",
        "            X_batch = X[i,:,:].reshape(self.minibatch_size, -1)\n",
        "            y_batch = y[i,:,:].reshape(self.minibatch_size, -1)           \n",
        "            # TODO: get y_pred using the forward pass\n",
        "            error_sum_train += self.get_error(self.forward_pass(X_batch), y_batch)\n",
        "        return error_sum_train / nbatch\n",
        "            \n",
        "        \n",
        "    def train(self, nepoch, X_train, y_train, X_test, y_test)-> float:\n",
        "        X_train = X_train.reshape(-1, self.minibatch_size, 784)\n",
        "        y_train = y_train.reshape(-1, self.minibatch_size, 10)\n",
        "        \n",
        "        X_test = X_test.reshape(-1, self.minibatch_size, 784)\n",
        "        y_test = y_test.reshape(-1, self.minibatch_size, 10)\n",
        "        \n",
        "        # TODO: Get the number of batch based on X_train's shape\n",
        "        nbatch = X_train.shape[0]\n",
        "        error_test = 0.0\n",
        "        for epoch in range(0, nepoch):\n",
        "            error_sum_train = 0.0\n",
        "            for i in range(0, nbatch):\n",
        "                X_batch = X_train[i,:, :]\n",
        "                y_batch = y_train[i,:, :]\n",
        "        \n",
        "                y_pred = self.forward_pass(X_batch)\n",
        "                self.backward_pass(y_pred - y_batch)\n",
        "                self.update_all_weights()\n",
        "                error_sum_train += self.get_error(y_pred, y_batch)\n",
        "            error_test = self.get_test_error(X_test, y_test)\n",
        "            print(f\"Training accuracy: {error_sum_train / nbatch:.3f}, Test accuracy: {error_test:.3f}\")\n",
        "        return error_test"
      ],
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn_WxionuGxU"
      },
      "source": [
        "## Training phase (12 pts)\n",
        "\n",
        "Now, it is time to train the model !!\n",
        "\n",
        "You can play with the different parameters (minibatch_size, nepoch, learning_rate and the number of hidden layers)\n",
        "\n",
        "It's on 12 points because there is a lot of functions to fill but also we want the training best training accuracy. \n",
        "\n",
        "To have all the point your neural network needs to have a Test accuracy > 92 % !! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CzUxULduGxW"
      },
      "source": [
        "minibatch_size = 20\n",
        "nepoch = 12\n",
        "learning_rate = 0.08\n",
        "\n",
        "ffnn = FFNN(config=[784, 50, 50, 10], minibatch_size=minibatch_size, learning_rate=learning_rate)"
      ],
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_qjf8WauGxf",
        "outputId": "3f5936ed-b49e-4510-e5a0-a77bf36e8ca2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "assert X_train.shape[0] % minibatch_size == 0\n",
        "assert X_test.shape[0] % minibatch_size == 0\n",
        "\n",
        "err = ffnn.train(nepoch, normalize_data(X_train), target_to_one_hot(y_train), normalize_data(X_test), target_to_one_hot(y_test))"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 0.844, Test accuracy: 0.910\n",
            "Training accuracy: 0.923, Test accuracy: 0.927\n",
            "Training accuracy: 0.940, Test accuracy: 0.935\n",
            "Training accuracy: 0.952, Test accuracy: 0.941\n",
            "Training accuracy: 0.957, Test accuracy: 0.944\n",
            "Training accuracy: 0.963, Test accuracy: 0.946\n",
            "Training accuracy: 0.968, Test accuracy: 0.945\n",
            "Training accuracy: 0.972, Test accuracy: 0.946\n",
            "Training accuracy: 0.974, Test accuracy: 0.946\n",
            "Training accuracy: 0.976, Test accuracy: 0.946\n",
            "Training accuracy: 0.979, Test accuracy: 0.948\n",
            "Training accuracy: 0.980, Test accuracy: 0.947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV6i-LCNuGxo"
      },
      "source": [
        "## Error analysis (2 pts)\n",
        "\n",
        "Here we use a subset of the test data to try and find some miss classification.\n",
        "\n",
        "It will help us understand why the neural network failed sometimes to classify images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bWQVO6WuGxr",
        "outputId": "6d417f24-a2fb-4d32-c7e1-5174140d569f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "nsample = 1000\n",
        "X_demo = X_test[:nsample]\n",
        "y_demo = ffnn.forward_pass(X_demo)\n",
        "y_true = y_test[:nsample]\n",
        "\n",
        "index_to_plot = 50 \n",
        "plot_one_image(X_demo, y_true, index_to_plot)\n",
        "\n",
        "# Compare to the prediction \n",
        "prediction = np.argmax(y_demo[index_to_plot])\n",
        "true_target = y_true[index_to_plot]\n",
        "# is it the same number ?"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wpa3KqeuGx1",
        "outputId": "bed16b54-e799-40b9-cab3-7f2201964a98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# loop arround the demo test set and try to find a miss prediction\n",
        "miss=0\n",
        "for i in range(0, nsample):   \n",
        "    prediction = str(np.argmax(y_demo[i])) # Todo\n",
        "    true_target = y_true[i] # Todo\n",
        "    if prediction != true_target:\n",
        "        # TODO\n",
        "        miss += 1\n",
        "print(miss)"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kPWkqZZuGx-"
      },
      "source": [
        "## Open analysis\n",
        "\n",
        "in the cell below please explain you choice for all the parameters of your configuration: \n",
        "\n",
        "- minibatch_size\n",
        "- nepoch\n",
        "- config\n",
        "- learning_rate\n",
        "\n",
        "Also explain how the neural network behave when changing them ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkWWMyRvuGyA"
      },
      "source": [
        "## Open analysis answer\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UESRXz8uGyB"
      },
      "source": [
        "#nepoch représente le nombre d'itérations mises en place pour entrainer le réseau\n",
        "\n",
        "#config : on peut faire varier le nombre de neurones du système ou les hidden layers en modifiant les deuxieme et troisième termes de la config\n",
        "\n",
        "#--> l'on peut déjà déduire de l'analyse de ces deux paramètres que l'ajout de plus de hidden layers ou de neurones nécessitera surement un plus grand nombre d'itérations nepoch,\n",
        "#mais l'on veut tout de même que le training ne soit pas trop long, on veut donc garder un nepoch bas.\n",
        "\n",
        "#learning_rate : le learning_rate alpha représente les sauts éffectués lors de la gradient descent. Moduler ce paramètre à donc pour but de jouer avec la \"taille\" des sauts lors de la descente de gradiant, \n",
        "#et l'on peut donc essayer en modulant ce paramètre de trouver avec plus de certitudes un minimum global et non local, vous avez dit en cours que l'on ne devait pas changer ce paramètre, mais une technique \n",
        "#comme l'annealing qui consiste à partir d'un learning rate élevé et à le réduire duran la descente semble appropriée pour s'echapper d'un minima local au profit d'un minima global\n",
        "\n",
        "#minibatch_size : dans un batch gradient descent, tout le training set est \"process\" à chaque itération, alors que dans une mini-batch gradient descent, on ne process qu'une petite partie du training set \n",
        "#à chaque itération, ainsi si l'on a un batch size de m, et un training set de taille n, pour un batch gradient descent, m=n, alors que pour un minibatch gradient descent, m=a avec a<n, il s'agit donc de \n",
        "#trouver la bonne quantité de données du training set à process à chaque itération.\n",
        "\n",
        "#On essayera de garder un nombre de hidden layers réduit et d'augmenter la minibatch_size  pour notre modèle.\n",
        "\n",
        "#minibatch_size = 20\n",
        "#nepoch = 12\n",
        "#learning_rate = 0.08\n",
        "#config=[784, 50, 50, 10]\n",
        "\n",
        "#Training accuracy: 0.980, Test accuracy: 0.947\n",
        "\n",
        "\n",
        "#Paramètres de fin, avec 60 samples missed"
      ],
      "execution_count": 323,
      "outputs": []
    }
  ]
}